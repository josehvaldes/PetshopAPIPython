{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from agent_state_lib import AgentState\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from language_lib import detect_language\n",
    "\n",
    "from milvus_lib import retrieve_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"h:/ML_Models/_gemma/model/gemma-2b-it\"  # local folder\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer based on retrieved documents.\"),\n",
    "    (\"human\", \"{question}\\n\\nDocs:\\n{retrieved_docs}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def generate_answer(state: AgentState) -> AgentState:\n",
    "    answer = chain.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"retrieved_docs\": \"\\n\".join(state[\"retrieved_docs\"])\n",
    "    })\n",
    "    return {**state, \"answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_state: AgentState = {\n",
    "    \"question\": \"\",\n",
    "    \"embedded_question\": \"big dogs\",}\n",
    "\n",
    "response = retrieve_docs(test_state)\n",
    "for doc in response[\"retrieved_docs\"]:\n",
    "    print(doc)\n",
    "\n",
    "def translate_question(state: AgentState) -> AgentState:\n",
    "    translation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Translate the following question to English.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    translator_chain = translation_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    translated = translator_chain.invoke({\"question\": state[\"question\"]})\n",
    "    return {**state, \"question\": translated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"detect_language\", detect_language)\n",
    "graph.add_node(\"translate\", translate_question)\n",
    "graph.add_node(\"retrieve\", retrieve_docs)\n",
    "graph.add_node(\"reason\", generate_answer)\n",
    "\n",
    "graph.set_entry_point(\"detect_language\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"detect_language\",\n",
    "    # This function returns a string key\n",
    "    detect_language,\n",
    "    # Routing map\n",
    "    {\n",
    "        \"english\": \"retrieve\",\n",
    "        \"non_english\": \"translate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"translate\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"reason\")\n",
    "graph.set_finish_point(\"reason\")\n",
    "\n",
    "agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\n",
    "    \"question\": \"based on the documents, what is the best dog for rescue operations?\",\n",
    "    \"embedded_question\": \"big dogs\",})\n",
    "print(response[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
